{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8292c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411f2d9",
   "metadata": {},
   "source": [
    "# **Bài 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13d617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số luật tìm được: 5\n",
      "    antecedents consequents  support  confidence      lift\n",
      "4       (MAGGI)       (TEA)      0.2    0.800000  2.285714\n",
      "2  (CORNFLAKES)    (COFFEE)      0.2    0.666667  1.666667\n",
      "3       (SUGER)    (COFFEE)      0.2    0.666667  1.666667\n",
      "0        (MILK)     (BREAD)      0.2    0.800000  1.230769\n",
      "1       (SUGER)     (BREAD)      0.2    0.666667  1.025641\n"
     ]
    }
   ],
   "source": [
    "file_path = 'GroceryStore-AssociateRules.txt'\n",
    "dataset = []\n",
    "with open(file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if line.strip() and '\\t' in line and ',' in line: # Lọc dòng hợp lệ\n",
    "            # Tách bỏ số thứ tự đầu dòng, lấy phần danh sách items\n",
    "            items = line.strip().split('\\t')[1].split(',')\n",
    "            dataset.append(items)\n",
    "\n",
    "# Chuyển đổi dữ liệu sang dạng One-Hot (TransactionEncoder)\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# chọn ngưỡng Support là 0.2 (20%) và Confidence là 0.6 (60%)\n",
    "# Lý do: Với 20 giao dịch, Support 0.2 nghĩa là sản phẩm phải xuất hiện ít nhất 4 lần.\n",
    "MIN_SUPPORT = 0.2\n",
    "MIN_CONFIDENCE = 0.6\n",
    "\n",
    "# Tìm các tập phổ biến\n",
    "frequent_itemsets = apriori(df, min_support=MIN_SUPPORT, use_colnames=True)\n",
    "\n",
    "# Sinh luật kết hợp\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=MIN_CONFIDENCE)\n",
    "\n",
    "# Sắp xếp theo độ mạnh (Lift) để dễ quan sát\n",
    "rules = rules.sort_values(by=['lift', 'confidence'], ascending=False)\n",
    "\n",
    "# Hiển thị kết quả\n",
    "print(\"Tổng số luật tìm được:\", len(rules))\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a290401",
   "metadata": {},
   "source": [
    "**Nhận xét:**\n",
    "**1. Cặp đôi tiềm năng nhất (Maggi & Tea):**\n",
    "Đây là luật mạnh nhất với **Lift = 2.28**. Khách hàng mua Mì gói (Maggi) có xu hướng rất cao (80%) sẽ mua thêm Trà. Cửa hàng nên xếp hai quầy này cạnh nhau.\n",
    "\n",
    "**2. Nhóm \"Bữa sáng\" (Coffee):**\n",
    "Cà phê thường được mua kèm với **Ngũ cốc (Cornflakes)** hoặc **Đường (Sugar)** với mức độ tương quan khá tốt (Lift ~ 1.67). Đây là combo bữa sáng tiêu chuẩn.\n",
    "\n",
    "**3. Mối quan hệ Sữa & Bánh mì:**\n",
    "Mặc dù phổ biến, nhưng mức độ thúc đẩy nhau (Lift = 1.23) chỉ ở mức trung bình, thấp hơn nhiều so với cặp Mì - Trà.\n",
    "\n",
    "**4. Luật yếu (Sugar & Bread):**\n",
    "Cặp Đường - Bánh mì có **Lift xấp xỉ 1 (1.02)**. Điều này chứng tỏ hai sản phẩm này gần như độc lập, khách mua cùng nhau chỉ là ngẫu nhiên chứ không phải do sản phẩm này kích thích sản phẩm kia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244381ec",
   "metadata": {},
   "source": [
    "# **Bài 2:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf5ccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tạo xong tập dữ liệu KHTN với 50179 thí sinh.\n",
      "   SBD_Moi    T   AV     V    Ly   Hoa  Sinh\n",
      "0        0  7.2  8.2  7.50  6.50  7.00  5.25\n",
      "1        1  8.2  5.0  6.00  5.50  7.25  4.75\n",
      "2        2  7.6  9.0  5.50  6.25  7.00  5.25\n",
      "3        3  7.8  9.2  8.00  5.75  6.75  5.25\n",
      "4        4  7.6  9.0  5.75  7.00  6.50  5.00\n",
      "\n",
      "--- Dữ liệu sau khi chuyển đổi 0/1 ---\n",
      "   SBD_Moi  T  AV  V  Ly  Hoa  Sinh\n",
      "0        0  0   1  0   0    0     0\n",
      "1        1  1   0  0   0    0     0\n",
      "2        2  0   1  0   0    0     0\n",
      "3        3  0   1  1   0    0     0\n",
      "4        4  0   1  0   0    0     0\n",
      "\n",
      "--- KẾT QUẢ LUẬT KẾT HỢP ---\n",
      "   antecedents consequents   support  confidence      lift\n",
      "8         (Ly)     (T, AV)  0.078200    0.687577  2.356044\n",
      "7     (Ly, AV)         (T)  0.078200    0.899587  2.078287\n",
      "2         (Ly)         (T)  0.099384    0.873839  2.018802\n",
      "10   (Hoa, AV)         (T)  0.084876    0.816996  1.887479\n",
      "3        (Hoa)         (T)  0.138066    0.745347  1.721951\n",
      "6      (T, Ly)        (AV)  0.078200    0.786846  1.630793\n",
      "4         (Ly)        (AV)  0.086929    0.764325  1.584116\n",
      "0          (T)        (AV)  0.291835    0.674217  1.397363\n",
      "1         (AV)         (T)  0.291835    0.604849  1.397363\n",
      "9     (T, Hoa)        (AV)  0.084876    0.614752  1.274116\n"
     ]
    }
   ],
   "source": [
    "file_path = 'ThiTNTHPT 2021-TpHCM.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df_khtn = df.dropna(subset=['Lý', 'Hoá', 'Sinh']).copy()\n",
    "\n",
    "cols_mapping = {\n",
    "    'Toán': 'T',\n",
    "    'Ngoại Ngữ': 'AV',\n",
    "    'Văn': 'V',\n",
    "    'Lý': 'Ly',\n",
    "    'Hoá': 'Hoa',\n",
    "    'Sinh': 'Sinh'\n",
    "}\n",
    "\n",
    "# Lọc lấy các cột điểm cần thiết\n",
    "df_processed = df_khtn[list(cols_mapping.keys())].copy()\n",
    "\n",
    "# Đổi tên cột theo yêu cầu đề bài (T-AV-V...)\n",
    "df_processed.rename(columns=cols_mapping, inplace=True)\n",
    "\n",
    "# Đánh số lại SBD để bảo mật (Xóa SBD cũ, tạo index mới)\n",
    "df_processed.reset_index(drop=True, inplace=True)\n",
    "df_processed.index.name = 'SBD_Moi' # Đặt tên index là SBD mới\n",
    "df_processed.reset_index(inplace=True) # Biến index thành cột thực\n",
    "\n",
    "print(\"Đã tạo xong tập dữ liệu KHTN với\", len(df_processed), \"thí sinh.\")\n",
    "print(df_processed.head())\n",
    "\n",
    "# Quy tắc: Điểm >= 8 thành 1, ngược lại thành 0\n",
    "# Lưu ý: Chỉ áp dụng lên các cột điểm (từ cột thứ 1 trở đi, bỏ cột SBD_Moi đầu tiên)\n",
    "score_cols = ['T', 'AV', 'V', 'Ly', 'Hoa', 'Sinh']\n",
    "df_binary = df_processed.copy()\n",
    "\n",
    "# Áp dụng logic thay thế\n",
    "df_binary[score_cols] = (df_binary[score_cols] >= 8).astype(int)\n",
    "\n",
    "print(\"\\n--- Dữ liệu sau khi chuyển đổi 0/1 ---\")\n",
    "print(df_binary.head())\n",
    "\n",
    "# KHAI PHÁ LUẬT KẾT HỢP (ASSOCIATION RULES) ---\n",
    "# 1. Tìm tập phổ biến (Frequent Itemsets)\n",
    "# min_support = 0.05: Môn/Combo môn đó phải xuất hiện ít nhất ở 5% thí sinh (Vì điểm >=8 khá khó nên để thấp)\n",
    "frequent_itemsets = apriori(df_binary[score_cols].astype(bool), min_support=0.05, use_colnames=True)\n",
    "\n",
    "# 2. Sinh luật (Rules)\n",
    "# min_threshold=0.5: Độ tin cậy ít nhất 50%\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "\n",
    "# Sắp xếp theo Lift giảm dần\n",
    "rules = rules.sort_values(by=['lift', 'confidence'], ascending=False)\n",
    "\n",
    "# Hiển thị kết quả\n",
    "print(\"\\n--- KẾT QUẢ LUẬT KẾT HỢP ---\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af924745",
   "metadata": {},
   "source": [
    "Sau khi xử lý và chuẩn hóa dữ liệu, tập KHTN còn **50.179 thí sinh**, đảm bảo đủ lớn để khai phá luật kết hợp. Việc chuyển điểm về dạng nhị phân (>=8 → 1, <8 → 0) cho thấy tỷ lệ điểm cao ở các môn tự nhiên không nhiều, nhưng vẫn xuất hiện nhiều mối quan hệ rõ rệt.\n",
    "\n",
    "Các luật có confidence rất cao cho thấy **thí sinh đạt điểm cao môn Lý thường đồng thời đạt điểm cao ở Toán và Anh Văn**, với luật (Ly → T, AV) có lift lên tới **2.35**, thể hiện mức liên hệ mạnh bất thường. Ngoài ra, tổ hợp (Ly, AV → T) và (Ly → T) đều có confidence trên 87%, cho thấy **năng lực học tốt môn Lý đi kèm với năng lực Toán rất rõ rệt**. Môn Hóa và môn Lý cũng góp phần dự đoán điểm cao môn Toán nhưng mức độ yếu hơn.\n",
    "\n",
    "Nhìn chung, các luật khai phá được phản ánh mối liên hệ tự nhiên giữa các môn trong tổ hợp KHTN: học sinh giỏi Lý có xu hướng giỏi Toán và Anh Văn, và các môn khoa học tự nhiên hỗ trợ nhau mạnh trong kết quả điểm cao.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0710d4",
   "metadata": {},
   "source": [
    "**Bài 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f1b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng bản ghi sau khi lọc: 83174\n",
      "        Area   Cuisine Grade\n",
      "0   BROOKLYN   Chinese     Z\n",
      "1  MANHATTAN  American     C\n",
      "3  MANHATTAN  American     A\n",
      "4  MANHATTAN  American     A\n",
      "5  MANHATTAN  American     C\n",
      "\n",
      "--- Top 10 Luật Kết Hợp mạnh nhất ---\n",
      "                              antecedents         consequents   support  \\\n",
      "28              (Grade_A, Cuisine_French)    (Area_MANHATTAN)  0.011374   \n",
      "4                        (Cuisine_French)    (Area_MANHATTAN)  0.020139   \n",
      "30              (Cuisine_French, Grade_B)    (Area_MANHATTAN)  0.005663   \n",
      "36            (Grade_C, Cuisine_Japanese)    (Area_MANHATTAN)  0.008031   \n",
      "33             (Cuisine_Italian, Grade_C)    (Area_MANHATTAN)  0.010135   \n",
      "42  (Cuisine_American, Area_STATENISLAND)           (Grade_A)  0.011157   \n",
      "19              (Grade_A, Area_MANHATTAN)  (Cuisine_American)  0.156972   \n",
      "32             (Grade_B, Cuisine_Italian)    (Area_MANHATTAN)  0.019441   \n",
      "8                     (Area_STATENISLAND)           (Grade_A)  0.021004   \n",
      "5                       (Cuisine_Italian)    (Area_MANHATTAN)  0.063517   \n",
      "\n",
      "    confidence      lift  \n",
      "28    0.809932  1.669109  \n",
      "4     0.797999  1.644519  \n",
      "30    0.759677  1.565545  \n",
      "36    0.715970  1.475473  \n",
      "33    0.676565  1.394267  \n",
      "42    0.673929  1.345948  \n",
      "19    0.647009  1.251933  \n",
      "32    0.638373  1.315562  \n",
      "8     0.630914  1.260040  \n",
      "5     0.616597  1.270684  \n",
      "\n",
      "--- Các luật liên quan đến 'Japanese' ---\n",
      "                        antecedents       consequents   support  confidence  \\\n",
      "36      (Grade_C, Cuisine_Japanese)  (Area_MANHATTAN)  0.008031    0.715970   \n",
      "6                (Cuisine_Japanese)  (Area_MANHATTAN)  0.041780    0.603194   \n",
      "35      (Grade_B, Cuisine_Japanese)  (Area_MANHATTAN)  0.013430    0.584205   \n",
      "34      (Grade_A, Cuisine_Japanese)  (Area_MANHATTAN)  0.018107    0.573933   \n",
      "40  (Area_QUEENS, Cuisine_Japanese)         (Grade_A)  0.005170    0.517449   \n",
      "\n",
      "        lift  \n",
      "36  1.475473  \n",
      "6   1.243064  \n",
      "35  1.203931  \n",
      "34  1.182763  \n",
      "40  1.033432  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('RestaurantDataset.csv', header=None, names=['Area', 'Cuisine', 'Grade'])\n",
    "\n",
    "# Xử lý chuỗi: Xóa khoảng trắng thừa ở đầu/cuối (trim whitespace)\n",
    "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "# Chỉ giữ lại các loại nhà hàng: Chinese, French, American, Italian, Japanese, Asian\n",
    "target_cuisines = ['Chinese', 'French', 'American', 'Italian', 'Japanese', 'Asian']\n",
    "df_filtered = df[df['Cuisine'].isin(target_cuisines)].copy()\n",
    "\n",
    "print(f\"Số lượng bản ghi sau khi lọc: {len(df_filtered)}\")\n",
    "print(df_filtered.head())\n",
    "\n",
    "df_encoded = pd.get_dummies(df_filtered, prefix=['Area', 'Cuisine', 'Grade'])\n",
    "\n",
    "# 1. Tìm tập phổ biến (Frequent Itemsets)\n",
    "# Min Support = 0.005 (0.5%): Vì ta muốn tìm các luật ngách \n",
    "frequent_itemsets = apriori(df_encoded.astype(bool), min_support=0.005, use_colnames=True)\n",
    "\n",
    "# 2. Sinh luật (Association Rules)\n",
    "# Min Confidence = 0.5 (50%): Chỉ lấy các luật có độ tin cậy cao\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "\n",
    "# Sắp xếp theo Lift (độ mạnh của luật) hoặc Confidence (độ tin cậy)\n",
    "rules_sorted = rules.sort_values(by=['confidence', 'lift'], ascending=False)\n",
    "\n",
    "# Chọn các cột quan trọng để hiển thị\n",
    "display_cols = ['antecedents', 'consequents', 'support', 'confidence', 'lift']\n",
    "print(\"\\n--- Top 10 Luật Kết Hợp mạnh nhất ---\")\n",
    "print(rules_sorted[display_cols].head(10))\n",
    "\n",
    "# --- KIỂM TRA CÁC LUẬT CỤ THỂ TRONG VÍ DỤ ---\n",
    "# Lọc ra các luật liên quan đến 'Japanese' để so sánh với đề bài\n",
    "print(\"\\n--- Các luật liên quan đến 'Japanese' ---\")\n",
    "japanese_rules = rules_sorted[rules_sorted['antecedents'].apply(lambda x: 'Cuisine_Japanese' in x)]\n",
    "print(japanese_rules[display_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bea7e7",
   "metadata": {},
   "source": [
    "**KẾT LUẬN NGẮN GỌN**\n",
    "\n",
    "Sau khi lọc giữ lại 6 nhóm nhà hàng, dữ liệu còn **83.174 bản ghi**, đủ lớn để khai phá luật kết hợp. Kết quả cho thấy xu hướng phân bố nhà hàng tại New York rất rõ rệt: **các nhà hàng French, Italian và Japanese có khả năng cao tập trung ở khu vực Manhattan**, thể hiện qua nhiều luật có confidence trên 75% và lift > 1.6. Ngược lại, **khu Staten Island có xu hướng xuất hiện nhiều nhà hàng được xếp hạng A**, với luật “Area_STATENISLAND ⇒ Grade_A” có support cao nhất trong top luật mạnh. Nhìn chung, các luật tìm được phản ánh đúng đặc trưng thực tế: Manhattan là khu vực tập trung nhiều nhà hàng nổi tiếng, trong khi Staten Island có chất lượng đánh giá vệ sinh tốt hơn.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
